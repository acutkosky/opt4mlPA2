{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "EC500 Opt4ML PA2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-rlonHRgR8e"
      },
      "source": [
        "# Optimization for Machine Learning PA 2\r\n",
        "\r\n",
        "This homework assignment investigates implementing some variants of Adam. We will be testing your optimizers on a simplified implementation of [GPT](https://github.com/openai/gpt-3) based on the [minGPT](https://github.com/karpathy/minGPT) repository by Andrej Karpathy. This is a model that takes as input a sequence of characters from a text file and attempts to predict the next character. This can be used to generate novel text by starting with a seed text string, and then repeatedly using the model to generate another character.\r\n",
        "\r\n",
        "There is only one place you need to write code in this notebook, for questions **1a**, **1b**.\r\n",
        "\r\n",
        "To turn in this homework: download as .ipynb (File -> download as .ipynb). Make the filename YOURNAME_PA2.ipynb and send via email attachment to opt4mlclass+program2@gmail.com with your name and PA2 in the subject line. Your submission should be saved with **all cells run to completion**. The final error of the best optimizer should be **less than 1.7** (and the best optimizer should be the one from question 1b).\r\n",
        "\r\n",
        "This homework is **DUE on Monday 3/22 at 11:59 pm**.\r\n",
        "\r\n",
        "# Tips\r\n",
        "* You will need a GPU for this assignment. When using google colab, go to runtime->change runtime type and make sure that the type is set to GPU.\r\n",
        "\r\n",
        "* You may decrease the number of training epochs while debugging, but please set it back to 20 and run again before submission.\r\n",
        "\r\n",
        "* Study the provided AdaGrad implementation closely, it introduces a few pytorch functions that may be useful. You should check the documentation for these functions to see what they do.\r\n",
        "\r\n",
        "* You may occasionally need to restart the runtime (runtime->restart runtime). Sometimes the GPUs don't release memory properly, and sometimes the progress bars get a little messed up.\r\n",
        "\r\n",
        "* If you do not run on google colab, you may need to run the commands in the first cell in a terminal in your working directory before running the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wzx2UJoruvmS"
      },
      "source": [
        "# grab the data and auxiliary code. Feel free to checkout the git repo to see\r\n",
        "# what the model code will do.\r\n",
        "!rm -r *\r\n",
        "!git clone https://github.com/acutkosky/opt4mlPA2.git\r\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\r\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZNclArxur0O"
      },
      "source": [
        "# set up logging\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPNiLzaiur0P"
      },
      "source": [
        "# make deterministic\n",
        "from opt4mlPA2.mingpt.utils import set_seed\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6-pL9SDur0Q"
      },
      "source": [
        "#import all the things\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "import math\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from opt4mlPA2.mingpt.model import GPT, GPTConfig\n",
        "from opt4mlPA2.mingpt.utils import sample\n",
        "from opt4mlPA2.mingpt.trainer import Trainer, TrainerConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayO9eAReur0R"
      },
      "source": [
        "# define a dataset class to process the textfile in pytorch.\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = list(set(data))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "        \n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.data) / (self.block_size + 1))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # we're actually going to \"cheat\" and pick a spot in the dataset at random\n",
        "        i = np.random.randint(0, len(self.data) - (self.block_size + 1))\n",
        "        chunk = self.data[i:i+self.block_size+1]\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QsEKavRur0S"
      },
      "source": [
        "# the \"block size\" is the number of characters the model takes as input.\r\n",
        "# in this case, it can look at up to 128 characters when predicting the next\r\n",
        "# character.\r\n",
        "block_size = 128 # spatial extent of the model for its context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk2o1Loxur0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9121b5-e425-4cf0-ada2-3f5ced1c4ef4"
      },
      "source": [
        "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
        "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
        "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 1115394 characters, 65 unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_2gClq9lDj_"
      },
      "source": [
        "# AdaGrad implementation\r\n",
        "This is a simple adagrad implementation. You can also checkout the official pytorch implementation [here](https://github.com/pytorch/pytorch/blob/master/torch/optim/adagrad.py).\r\n",
        "\r\n",
        "Take particular note of the functions `addcmul_` and `addcdiv_`. You may want to look up what they do for use in your solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpqcVSyeIpp4"
      },
      "source": [
        "class AdaGrad(Optimizer):\r\n",
        "  def __init__(self, params, lr=1.0, betas=(0.9,0.999), decouple=False, debias=True):\r\n",
        "    # betas are ignored, but we keep them in the function signature so that it is the same as\r\n",
        "    # the adam variants.\r\n",
        "    super(AdaGrad, self).__init__(params, {'lr': lr, 'beta1': betas[0], 'beta2': betas[1], 'weight_decay': 0.0})\r\n",
        "\r\n",
        "\r\n",
        "    for group in self.param_groups:\r\n",
        "      for p in group['params']:\r\n",
        "        state = self.state[p]\r\n",
        "        state['step'] = 0\r\n",
        "        state['v'] = torch.zeros_like(p, memory_format=torch.preserve_format)\r\n",
        "\r\n",
        "\r\n",
        "  @torch.no_grad()\r\n",
        "  def step(self, closure=None):\r\n",
        "    # in this class, and also usually in practice, closure will always be None.\r\n",
        "    loss = None\r\n",
        "    epsilon = 1e-8\r\n",
        "    if closure is not None:\r\n",
        "      with torch.enable_grad():\r\n",
        "        loss = closure()\r\n",
        "\r\n",
        "    for group in self.param_groups:\r\n",
        "      lr = group['lr']\r\n",
        "      beta1 = group['beta1']\r\n",
        "      beta2 = group['beta2']\r\n",
        "      weight_decay = group['weight_decay']\r\n",
        "\r\n",
        "      # it is common practice to call the model parameters p in code.\r\n",
        "      # in class we follow more closely analytical conventions, in which the\r\n",
        "      # parameters are often called w for weights.\r\n",
        "      for p in group['params']:\r\n",
        "        if p.grad is None:\r\n",
        "          continue\r\n",
        "\r\n",
        "        if weight_decay != 0.0:\r\n",
        "          p.grad.add_(p, alpha=weight_decay)\r\n",
        "        \r\n",
        "        # Update the iteration counter (again, this is not actually used in this algorithm)\r\n",
        "        state = self.state[p]\r\n",
        "        state['step'] += 1\r\n",
        "\r\n",
        "\r\n",
        "        state['v'].addcmul_(p.grad, p.grad, value=1.0)\r\n",
        "\r\n",
        "        p.addcdiv_(p.grad, torch.sqrt(state['v']).add_(epsilon), value=-lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIFg4v93Z_q8"
      },
      "source": [
        "# QUESTION 1a\r\n",
        "\r\n",
        "Implement the [AdamW](https://openreview.net/pdf?id=Bkg6RiCqY7) update *without* using the debiasing terms. AdamW performs the following (per-coordinate) update:\r\n",
        "\r\n",
        "$$\r\n",
        "w_{t+1} = w_t - \\eta_t\\left(\\frac{\\hat m_t}{\\sqrt{\\hat v_t} +\\epsilon} + \\lambda w_t\\right)\r\n",
        "$$\r\n",
        "where $\\hat m_t$ and $\\hat v_t$ are generated the same way as in the standard [Adam](https://openreview.net/pdf?id=Bkg6RiCqY7) update, and $\\lambda$ is an extra \"weight decay\" parameter provided to the optimizer. \r\n",
        "\r\n",
        "Ordinarily, \"weight decay\" is another word for L2 regularization. That is, the loss is modified to:\r\n",
        "$$\r\n",
        "\\mathcal{L}(w) + \\frac{\\lambda}{2}\\|w\\|^2\r\n",
        "$$\r\n",
        "This means that we could implement weight decay by changing the gradient to $\\nabla \\mathcal{L}(w) + \\lambda w$. The idea behind AdamW is that the weight-decay term is in some sense \"well-understood\" and should not be included in the $v_t$ and $A_t$ statistics that are being used to understand the more mysterious loss surface $\\mathcal{L}(w)$. See the linked paper for more details and full pseudocode.\r\n",
        "\r\n",
        "In your implementation, you should use the raw $m_t$ and $v_t$ values without applying the debiasing terms discussed in the papers and class.\r\n",
        "\r\n",
        "# QUESTION 1b\r\n",
        "\r\n",
        "Upgrade your debias-free AdamW implementation to use the `use_norm_scaling` argument of the `__init__` method. When this argument is `True`, you should scale the learning rate by the norm of the weights *for the given pytorch variable*. That is, for each variable $p$ you will replace the learning rate $\\eta_t$ at time $t$ with $\\|p\\|\\eta_t$ in the update:\r\n",
        "$$\r\n",
        "w_{t+1}[i] = w_t[i] - \\|w_t\\|_2\\eta_t\\left(\\frac{m_t[i]}{\\sqrt{v_t[i]} +\\epsilon} + \\lambda w_t[i]\\right)\r\n",
        "$$\r\n",
        "When the `use_norm_scaling` argument is false, simply perform the update from question 1a.\r\n",
        "\r\n",
        "This learning rate heuristic is inspired by a similar proposal for use with normalized updates in the [LARS](https://arxiv.org/abs/1708.03888) optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNST4-QUKyTc"
      },
      "source": [
        "\r\n",
        "class AdamW_bias(Optimizer):\r\n",
        "  def __init__(self, params, lr=1.0, betas=(0.9,0.999), use_norm_scaling=False):\r\n",
        "    super(AdamW_bias, self).__init__(params, {'lr': lr, 'beta1': betas[0], 'beta2': betas[1], 'weight_decay': 0.0})\r\n",
        "\r\n",
        "    self.use_norm_scaling = use_norm_scaling\r\n",
        "\r\n",
        "    for group in self.param_groups:\r\n",
        "      for p in group['params']:\r\n",
        "        ## YOUR CODE HERE ##\r\n",
        "\r\n",
        "\r\n",
        "  @torch.no_grad()\r\n",
        "  def step(self, closure=None):\r\n",
        "    # in this class, and also usually in practice, closure will always be None.\r\n",
        "    loss = None\r\n",
        "    epsilon = 1e-8\r\n",
        "    if closure is not None:\r\n",
        "      with torch.enable_grad():\r\n",
        "        loss = closure()\r\n",
        "      \r\n",
        "    ## YOUR CODE HERE ##\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG99O9rXur0U"
      },
      "source": [
        "# generate the configuration for the model. These parameters specify\n",
        "# the neural network architecture we will be using. It is not necessary\n",
        "# to understand this.\n",
        "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
        "                  n_layer=8, n_head=8, n_embd=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPP9QE90E1Aa"
      },
      "source": [
        "# generate training configurations for each of the optimizers. We will be testing\r\n",
        "# adagrad\r\n",
        "# adam (official pytorch implementation)\r\n",
        "# adamw (official pytorch implementation)\r\n",
        "# your optimizer both with and without the norm_scaling flag set.\r\n",
        "def adamw_bias_factory(params, lr, betas):\r\n",
        "  return AdamW_bias(params, lr, betas)\r\n",
        "\r\n",
        "def adamw_bias_norm_scaling_factory(params, lr, betas):\r\n",
        "  return AdamW_bias(params, lr, betas, use_norm_scaling=True)\r\n",
        "\r\n",
        "optimizers = {\r\n",
        "    'adagrad': AdaGrad, \r\n",
        "    'adam': torch.optim.Adam, \r\n",
        "    'adamw': torch.optim.AdamW, \r\n",
        "    'adamw_bias': adamw_bias_factory, \r\n",
        "    'adamw_bias_norm_scaling': adamw_bias_norm_scaling_factory\r\n",
        "  }\r\n",
        "\r\n",
        "training_configs = {}\r\n",
        "\r\n",
        "for name, opt in optimizers.items():\r\n",
        "# construct a training config: this sets the learning rate, batch size, number \r\n",
        "# of epochs ect for each optimizer. warmup_tokens and final_tokens are parameters\r\n",
        "# used to setup a warm-up and decay learning rate scheduler.\r\n",
        "  training_configs[name] = TrainerConfig(max_epochs=20, batch_size=256, learning_rate=6e-4, optimizer=opt,\r\n",
        "                        lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(train_dataset)*block_size,\r\n",
        "                        num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGDVS4_ZQNRV"
      },
      "source": [
        "# train a model on each optimizer, keeping track of the best-performing one.\r\n",
        "losses = {}\r\n",
        "min_loss = float('inf')\r\n",
        "best_model = None\r\n",
        "best_optimizer = None\r\n",
        "for name, tconf in training_configs.items():\r\n",
        "  print(\"training new model with optimizer: {}\".format(name))\r\n",
        "  model = GPT(mconf)\r\n",
        "  trainer = Trainer(model, train_dataset, None, tconf)\r\n",
        "  train_loss = trainer.train()\r\n",
        "  losses[name] = train_loss\r\n",
        "  train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters\r\n",
        "  print(\"final epoch train loss: {}\".format(train_loss))\r\n",
        "  if train_loss < min_loss:\r\n",
        "    best_model = model\r\n",
        "    best_optimizer = name\r\n",
        "    min_loss = train_loss\r\n",
        "\r\n",
        "print(\"best optimizer: {} with loss: {}\".format(best_optimizer, min_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PeDjCTOur0V"
      },
      "source": [
        "# alright, let's sample some character-level shakespear\n",
        "\n",
        "context = \"O God, O God!\"\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "y = sample(best_model, x, 2000, temperature=0.9, sample=True, top_k=5)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print(completion)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}